---
title: "Ch 6: Confidence Interval"
subtitle: "STAT 218 - Week 4, Lecture 2"
date: today
date-format: "MMMM D[<sup style='font-size:65%;'>th</sup>], YYYY"
format: 
  revealjs:
      theme: slide-style.scss
      slide-number: true
      chalkboard: true
      incremental: true
---

## Some Announcements

::: {style="font-size: 32px"}

- Quiz answer keys are available on Canvas   
- There is an opportunity if you are interested in.
  - [Funded summer program in biostatistics and data science](https://www.stat.uci.edu/isi-buds/)
  - Applications due - March 15   
- Check-in survey before the exam (5 minutes)
  - Go to our Canvas page and participate in the survey (It is under the section of Quizzes)


:::

# Central Limit Theorem

## 
### Definition
::: {style="font-size: 32px"}

- The Central Limit Theorem states that, __no matter what distribution Y may have__ in the population, if the sample size is _large enough_, then the sampling distribution of $\bar{Y}$ will be approximately a normal distribution.

- The significance of the Central Limit Theorem lies in its applicability when the shape of the population distribution is unknown, a common scenario in practical situations. 

- __How large__ a sample size is required?

  - For a normal distribution, any $n$ suffices. In the case of m
  - Fo moderately nonnormal distributions, a moderate $n$ is adequate. 
  - For highly nonnormal distributions, a rather large $n$ becomes necessary.

:::

## Comparison of Distributions
::: {style="font-size: 24px"}

```{r}
#| echo: false
# Set seed for reproducibility
set.seed(43)

# 1. Population Distribution (e.g., Normal Distribution)
population <- rnorm(10000, mean = 0, sd = 1)

# 2. A Sample (n) from the Population
sample_size <- 70
sample <- sample(population, size = sample_size)

# 3. Distribution of All Possible Sample Means of n
num_samples <- 1000
sample_means <- numeric(num_samples)

for (i in 1:num_samples) {
  sample_means[i] <- mean(sample(population, size = sample_size))
}

# Plotting the Distributions
par(mfrow = c(1, 3))

# Population Distribution
hist(population, main = "Population Distribution", xlab = "Value", col = "lightblue", border = "black")

# Sample Distribution
hist(sample, main = "A Random Sample from that Population", xlab = "Value", col = "lightgreen", border = "black")

# Distribution of Sample Means
hist(sample_means, main = "Distribution of Sample Means", xlab = "Sample Mean", col = "lightcoral", border = "black")

```


:::

## Comparison of Distributions
::: {style="font-size: 24px"}


```{r}
#| echo: false
# Set seed for reproducibility
set.seed(4)

# 1. Population Distribution (e.g., Normal Distribution)
population <- rexp(1000)

# 2. A Sample (n) from the Population
sample_size <- 30
sample <- sample(population, size = sample_size)

# 3. Distribution of All Possible Sample Means of n
num_samples <- 1000
sample_means <- numeric(num_samples)

for (i in 1:num_samples) {
  sample_means[i] <- mean(sample(population, size = sample_size))
}

# Plotting the Distributions
par(mfrow = c(1, 3))

# Population Distribution
hist(population, main = "Population Distribution", xlab = "Value", col = "lightblue", border = "black")

# Sample Distribution
hist(sample, main = "A Random Sample from that Population", xlab = "Value", col = "lightgreen", border = "black")

# Distribution of Sample Means
hist(sample_means, main = "Distribution of Sample Means", xlab = "Sample Mean", col = "lightcoral", border = "black")

```


:::


# Statistical Estimation

## Statistical Estimation

::: {style="font-size: 30px"}

- We assume that our data set is a __random sample__ from __some population__ 
  - This assumption enables us to use the information in that sample to infer facts about the population. 
  
<br>

- __Statistical estimation__ is a form of statistical inference in which we use the data to 
  (1) determine an estimate of some feature of the population and 
  (2) assess the precision of the estimate.

:::

##
### An Example for Statistical Estimation

::: {.columns}
::: {.column width="50%"}
::: {style="font-size: 28px"}

Let's have a look wing areas of 14 male Monarch butterflies at Oceano Dunes State Park in California


- $\bar{y} = 32.81$ $cm^2$  and $s= 2.48$ $cm^2$

::: fragment
Suppose we consider these 14 observations as a random sample from a population. 
:::

- $\mu$ = the (population) mean wing area of male Monarch butterflies in the Oceano
Dunes region
- $\sigma$ = the (population) SD of wing area of male Monarch butterflies in the Oceano
Dunes region

:::
:::

::: {.column width="50%"}
<a title="© Derek Ramsey / derekramsey.com" href="https://commons.wikimedia.org/wiki/File:Monarch_Butterfly_Showy_Male_3000px.jpg"><img width="1024" alt="Monarch Butterfly Showy Male 3000px" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/Monarch_Butterfly_Showy_Male_3000px.jpg/1024px-Monarch_Butterfly_Showy_Male_3000px.jpg"></a>


::: fragment
::: {style="font-size: 28px"}
From the sample data we have, we can say that  

- 32.81 is an estimate of __$\mu$__.
- 2.48 is an estimate of __$\sigma$__.
:::
:::
:::
:::

## Statistical Estimation cont.d
::: {style="font-size: 25px"}

We should be aware of the fact that these estimates are subject to sampling error.

::: fragment
::: callout-warning
- This is NOT measurement error; 
  - No matter how accurately each individual butterfly was measured, the sample information is imperfect 
    - due to the fact that only 14 butterflies were measured, rather than the entire population of butterflies.

:::
:::

- Broadly speaking, for a sample of observations on a quantitative variable _Y_
  - $\bar{y}$ is an estimate of __$\mu$__.
  - _s_ is an estimate of __$\sigma$__.
  
::: fragment
And our goal is to estimate __$\mu$__.
:::
:::

## Standard Error of the Mean
::: {style="font-size: 28px"}

__Sampling Error:__ the amount of discrepancy between $\bar{y}$ and $\mu$ is described (in a probability sense) by the sampling distribution of $\bar{Y}$


- As _s_ is an estimate of __$\sigma$__, a natural estimate of $\sigma / \sqrt{n}$ is $s/\sqrt{n}$

<br/><br/>


__The standard error of the mean__ is defined as follow:

$$
SE_\bar{Y} = \frac{s}{\sqrt{n}}
$$

:::

## Standard Error of the Mean cont.d
::: {style="font-size: 28px"}

- SE is an estimate of $\sigma_\bar{y}$. 
  - It can be interpreted in terms of the expected sampling error: 
    - In a broader sense, the difference between $\bar{y}$ and $\mu$ is rarely more than a few standard errors. 
    - Not surprisingly, we expect $\bar{y}$ to be within about one standard error of $\mu$.
    - Thus, the smaller the SE, the more precise the estimate. 
    - And, sample size is a factor that affect the magnitude of SE
- Remember __Example 5.2.2__ 
    
:::
    
##
### Standard Error vs Standard Deviation
::: {style="font-size: 28px"}

- These two quantities describe entirely different aspects of the data.
  - The SD describes the dispersion of the data, 
  - SE describes the unreliability (due to sampling error) in the mean of the sample.

- As the sample size increases, 
  - The sample mean and SD tend to approach more closely the population mean and SD
  - The standard error, by contrast, tends to decrease as n increases; when n is very large, the SE is very small and
    - so the  sample mean is a very precise estimate of the population mean.
:::



# Confidence Interval

  
##
### An Introduction to Confidence Interval - I

::: {.columns}
::: {.column width="50%"}
::: {style="font-size: 28px"}

- Our aim is to...
  - determine an estimate of $\mu$
    - $\bar{y}$ was an estimate of $\mu$
 
- We also know that the difference between $\bar{y}$ and $\mu$ is rarely more than a few standard errors.

::: fragment
:::{.callout-tip}

If $Z$ is a standard normal random variable, then the probability that $Z$ is between $\pm$ 2 is about 0.95 (OR 95% if we remember The 68/95/99.7 rule)
:::
:::

:::
:::

::: {.column width="50%"}
::: {style="font-size: 28px"}
::: fragment

![](img/68-95-99.png)
:::
:::
:::
:::

##
### An Introduction to Confidence Interval - II


To understand how to calculate confidence intervals, we need to have

- standardization formula
- standard error
- Z Score Table (Table 3 in our book) and 
- solve an equation that composed of these 


## 
### An Introduction to Confidence Interval - III

::: {style="font-size: 28px"}

- Let's calculate 95% confidence interval

::: fragment
:::{.callout-tip}

If $Z$ is a standard normal random variable, then the probability that $Z$ is between $\pm$ 2 is about 95% (Remember The 68/95/99.7 rule)
:::

:::

::: fragment

$$
Pr\{ -1.96 < \frac{\bar{Y}-\mu}{\sigma/\sqrt{n}} < 1.96 \} =0.95
$$

:::

::: fragment
If you solve this, it will become
:::

::: fragment
::: {style="text-align: center"}


$$
95 \% \ CI = (\bar{Y} \pm 1.96 \sigma / \sqrt{n})
$$

:::
:::
:::

##
#### A Further Example from Your Weekly Assignment

::: {style="font-size: 28px"}

__Exercise 5.2.8__ The heights of a certain population of corn plants follow a normal distribution with mean 145 cm and standard deviation 22 cm. We collected data from 16 plants and calculated the sample mean as 135 cm.

If $\bar{Y}$ represents the mean height of a random sample of 16 plants from the population (which is 135), 95% confidence interval (CI) for $\mu$ can be calculated as following:


::: fragment
::: {style="text-align: center; font-size: 28px"}
$$
95 \% \ CI = (\bar{Y} \pm 1.96 \ X\ \sigma / \sqrt{n})
$$
:::
:::fragment
$$
= (135\pm 1.96 \ X \ 22 / \sqrt{16})  
$$
:::
::: fragment
$$
=(124.22,145.78)
$$
:::
:::
:::

## 
### Understanding Confidence Intervals

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
options(scipen=0)
library(tidyverse)
library(fivethirtyeight)
library(scales)
```
::: {style="font-size: 28px"}

To help you visualize, imagine we have a population, and from that population, we randomly select a group of 20 observational unit

```{r fig.height = 4, fig.align='center'}
set.seed(7)

bike_prop <- 0.15

bike <- c(rep("yes", bike_prop*10000), rep("no", (1-bike_prop)*10000))

```

::: fragment
95%CI = (-44.47, 20.13)
:::

::: fragment

```{r fig.height = 4, fig.align='left'}

tibble(p = bike_prop, n = 1, lower_bound = -44.47, upper_bound = 20.13) %>% 
  ggplot() +
  aes(x = p, y = n) +
  geom_point() +
  geom_errorbarh(aes(xmin = lower_bound, xmax = upper_bound, height = .01)) +
  labs(y = " ",
       x = " ") +
  theme(axis.text.y = element_blank()) 
```
:::
:::

##
### Understanding Confidence Intervals

::: {style="font-size: 28px"}

 If we repeat this process 100 times, creating 100 different samples of 20 customers each, we would end up with 100 different samples drawn from the population.

::: fragment
![](img/fig-6.PNG){fig-align="center" width="100%"}
:::
:::

##
### Understanding Confidence Intervals

::: {style="font-size: 24px"}
If we calculate confidence intervals for each of these 100 samples, we will find that... 

::: fragment

![](img/fig-7.PNG){fig-align="center" width="100%"}
:::

::: fragment
- Around 95% of these intervals capture the true population mean difference between the two groups
- We are 95% confident that the true population mean difference is in this confidence interval.
:::
:::



##
### Confidence Interval - Verbal Explanation
::: {style="font-size: 24px"}

And...

::: fragment

- If we calculate confidence intervals for each of these 100 samples, we will find that around 95% of these intervals capture the true population mean difference between the two groups. 

- We are 95% confident that the true population mean difference is in this confidence interval.

:::
:::

##
### Confidence Interval and Sampling Distribution

![](img\ci-and-sampling.png){fig-align="center"}